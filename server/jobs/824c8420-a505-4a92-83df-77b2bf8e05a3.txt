This video is going to give you everything you need to go from knowing absolutely nothing about artificial intelligence and large language models to having a solid foundation of how these revolutionary technologies work. Over the past year, artificial intelligence has completely changed the world, with products like ChatGPT potentially appending every single industry and how people interact with technology in general. And in this video, I will be focusing on LLMs, how, how they work, ethical considerations, applications, and so much more. And this video was created in collaboration with an incredible program called AI Camp, in which high school students learn all about artificial intelligence. And I'll talk more about that later in the video. Let's go. So first, what is an LLM? Is it different from AI? And how is ChatGPT related to all of this? LLMs stand for large language models, which is a type of neural network that's trained on massive amounts of text data. It's generally trained on data that can be found online, everything from web scraping to books to transcripts. Anything that is text based can be trained into a large language model. And taking a step back, what is a neural network? A neural network is essentially a series of algorithms that try to recognize patterns in data. And really what they're trying to do is simulate how the human brain works. And LLMs are a specific type of neural network that focus on understanding natural language. And as mentioned, LLMs learn by reading tons of books, articles, Internet text, and there's really no limitation there. And so how do LLMs differ from traditional programming? Well, with traditional programming, it's instruction based, which means if X, then Y, you're explicitly telling the computer what to do. You're giving it a set of instructions to execute. But with LLMs, it's a completely different story. You're teaching the computer not how to do things, but how to learn how to do things. And this is a much more flexible approach and is really good for a lot of different applications where previously traditional coding could not accomplish them. So one example application is image recognition. With image recognition, traditional programming would require you to hard code every single rule for how to, let's say, identify different letters. So A, B, C, D. But if you're handwriting these letters, everybody's handwritten letters look different. So how do you use traditional programming to identify every single possible variation? Well, that's where this AI approach comes in. Instead of giving a computer explicit instructions for how to identify a handwritten letter, you instead give it a bunch of examples of what handwritten letters look like, and then it can infer what a new handwritten letter looks like based on all of the examples that it has. What also sets machine learning and large language models apart, and this new approach to programming is that they are much more flexible, much more adaptable, meaning they can learn from their mistakes and inaccuracies and are thus so much more scalable than traditional programming. LLMs are incredibly powerful at a wide range of tasks, including summarization, text generation, creative writing, question and answer programming, and if you've watched any of my videos, you know how powerful these large language models can be. And, and they're only getting better. Know that right now, large language models and AI in general are the worst they'll ever be. And as we're generating more data on the Internet, and as we use synthetic data, which means data created by other large language models, these models are going to get better rapidly. And it's super exciting to think about what the future holds. Now let's talk a little bit about the history and evolution of large language models. We're going to cover just a few of the large language models today in this section. The history of LLMs traces all the way back to the ELIZA model, which was from 1966, which was really the first language model. It had pre programmed answers based on keywords, it had a very limited understanding of the English language. And like many early language models, you started to see holes in its logic after a few back and forths in a conversation. And then after that, language models really didn't evolve for a very long time. Although technically the first recurrent neural network was created in 1924 or RNN, they weren't able to learn until 1972. And these new learning language models are a series of neural networks with layers and weights and a whole bunch of stuff that I'm not going to get into in this video. And RNNs were really the first technology that was able to predict the next word in a sentence rather than having everything pre programmed for it. And that was really the basis for how current large language models work. And even after this and the advent of deep learning in the early 2000s, the field of AI evolved very slowly, with Langu language models far behind what we see today. This all changed in 2017 where the Google DeepMind team released a research paper about a new technology called transformers. And this paper was called attention is all you need. And a quick side note, I don't think Google even knew quite what they had published at that time, but that same paper is what led OpenAI to develop ChatGPT. So obviously other computer scientists saw the potential for the Transformers architecture With this new Transformers architecture, it was far more advanced, it required decreased training time and it had many other features like self attention which I'll cover later in this video. Transformers allowed for pre trained large language models like GPT1 which was developed by OpenAI in 2018. It had 117 million parameters and it was completely revolutionary, but soon to be outclassed by other LLMs. Then after that, Bert was released. Bert in 2018 that had 340 million parameters and had bidirectionality, which means it had the ability to process text in both directions, which helped it have a better understanding of context. And as comparison, a unidirectional model only has an understanding of the words that came before the target text. And after this LLMs didn't develop a lot of new technology, but they did increase greatly in scale. GPT2 was released in early 2019 and had 2.5 billion billion parameters. Then GPT3 in June of 2020 with 175 billion parameters. And it was at this point that the public started noticing large language models. GPT had a much better understanding of natural language than any of its predecessors. And this is the type of model that powers ChatGPT, which is probably the model that you're most familiar with. And ChatGPT became so popular because it was so much more accurate than anything anyone had ever seen before. And it was really because of its size and because it was now buil into this chatbot format, anybody could jump in and really understand how to interact with this model. ChatGPT 3.5 came out in December of 2022 and started this current wave of AI that we see today. Then in March 2023, GPT 4 was released and it was incredible and still is incredible to this day. It had a whopping reported 1.76 trillion parameters and uses likely a mixture of experts approach, which means it has multiple models that are all fine tuned for specific cases. And then when somebody asks a question to it, it chooses which of those models to use. And then they added multimodality and a bunch of other features. And that brings us to where we are today. All right, now let's talk about how LLMs actually work in a little bit more detail. The process of how large language models work can be split into three steps. The first of these steps is called tokenization. And there are neural networks that are trained to split long text into individual tokens. And a token is essentially about 3/4 of a word. So if it's a short word like hi or that or there it's probably just one token. But if you have a longer word like summarization, it's going to be split into multiple pieces. And the way that tokenization happens is actually different for every model. Some of them separate prefixes and suffixes. Let's look at an example. What is the tallest building? So what is the tallest building? Are all separate tokens. And so that separates the suffix off of tallest but not building, because it is taking the context into account. And this step is done so models can understand each word individually. Just like humans, we understand each word individually and as groupings of words. And then the second step of LLMs is something called embeddings. The large language models turns those tokens into embedding vectors, turning those tokens into essentially a bunch of numerical representations of those tokens, numbers. And this makes it significantly easier for the computer to read and understand each word and how the different words relate to each other. And these numbers all correspond with the and embeddings vector database. And then the final step in the process is transformers, which we'll get to in a little bit. But first let's talk about vector databases. And I'm going to use the terms word and token interchangeably. So just keep that in mind, because they're almost the same thing. Not quite, but almost. And so these word embeddings that I've been talking about are placed into something called a vector database. These databases are storage and retrieval mechanisms that are highly optimized. And for vectors, and again, those are just numbers, long series of numbers. Because they're converted into these vectors, they can easily see which words are related to other words based on how similar they are, how close they are based on their embeddings. And that is how the large language model is able to predict the next word based on the previous words. Vector databases capture the relationship between data as vectors in multi dimensional space. I know that sounds complicated, but it's really just a lot of numbers. Vectors are objects with a magnitude and a direction which both influence how similar one vector is to another. And that is how LLMs represent words based on those numbers. Each word gets turned into a vector, capturing semantic meaning and its relationship to other words. So here's an example. The words book and worm, which independently might not look like they're related to each other, but they are related concepts because they frequently appear together. A bookworm, somebody who likes to read a lot. And because of that, they will have embeddings that close to each other. And so models build up an Understanding of natural language using these embeddings and looking for similarity of different words, terms, groupings of words, and all of these nuanced relationships. And the vector format helps models understand natural language better than other formats. And you can kind of think of all this like a map. If you have a map with two landmarks that are close to each other, they're likely going to have very similar coordinates. So it's kind of like that. Okay, now let's talk about transformers. Matrix representations can be made out of those vectors that we were just talking about. This is done by extracting some information out of the numbers and placing all of the information into a matrix through an algorithm called multi head attention. The output of the multi head attention algorithm is a set of numbers, which tells the model how much the words and its order are contributing to the sentence as a whole. We transform the input matrix into an output matrix, which will then correspond with a word having the same values as that output matrix. So basically, we're taking that input matrix, converting it into an output matrix, and then converting it into natural language, and the word is the final output of this whole process. This transformation is done by the algorithm that was created during the training process. So the model's understanding of how to do this transformation is based on all of its knowledge that it was trained with all of that text data from the Internet, from books, from articles, et cetera. And it learned which sequences of words go together and their corresponding next words based on the weights determined during training. Transformers use an attention mechanism to understand the context of words within a sentence. It involves calculations with the dot product, which is essentially a number representing how much the word contributed to the sentence. It will find the difference between the dot products of words and give it correspondingly large values for attention. And it will take that word into account more if it has higher attention. Now, let's talk about how large language models actually get trained. The first step of training a large language model is collecting the data. You need a lot of data. When I say billions of parameters, that is just a measure of how much data is actually going into training these models. And you need to find a really good data set. If you have really bad data going into a model, then you're going to have a really bad model. Garbage in, garbage out. So if a data set is incomplete or biased, the large language model will be also. And data sets are huge. We're talking about massive, massive amounts of data. They take data in from web pages, from books, from conversations, from Reddit, posts, from x posts, from YouTube, transcriptions, basically anywhere where we can get some text data. That data is becoming so valuable. Let me put into context how massive the data sets we're talking about really are. So here's a little bit of text, which is 276 tokens. That's it. Now, if we zoom out that one pixel is that many tokens. And now here's a representation of 285 million tokens, which is 0.02% of the 1.3 trillion tokens that some large language models take to train. And there's an entire science behind data pre processing, which prepares the data to be used to train a model. Everything from looking at the data quality to labeling, consistency, data cleaning, data transformation, and data reduction. But I'm not going to go too deep into that. And this pre processing can take a long time, and it depends on the type of machine being used, how much processing power you have, the size of the data set, the number of pre processing steps, and a whole bunch of other factors that make it really difficult to know exactly how long pre processing is going to take. But one thing that we know takes a long time is the actual training. Companies like Nvidia are building hardware specifically tailored for the math behind large language models. And this hardware is constantly getting better. The software used to process these models are getting better also. And so the total time to process models is decreasing, but the size of the models is increasing. And to train these models, it is extremely expensive because you need a lot of processing power, electricity, and these chips are not cheap. And that is why Nvidia stock price has skyrocketed. Their revenue growth has been extraordinary. And so with the process of training, we take this pre process text data that we talked about earlier, and it's fed into the model model. And then using transformers or whatever technology a model is actually based on, but most likely transformers, it will try to predict the next word based on the context of that data. And it's going to adjust the weights of the model to get the best possible output. And this process repeats millions and millions of times over and over again until we reach some optimal quality. And then the final step is evaluation. A small amount of the data is set aside for evaluation, and the model is tested on this data set for performance. And then the model is adjusted if necessary. The metric used to determine the effectiveness of the model is called perplexity. It will compare two words based on their similarity, and it will give a good score if the words are related and a bad score if it's not. And then we Also use RLHF reinforcement learning through human feedback. And that's when users or testers actually test the model and provide positive or negative scores based on the output. And then once again, again the model is adjusted as necessary. All right, let's talk about fine tuning now, which I think a lot of you are going to be interested in because it's something that the average person can get into quite easily. So we have these popular large language models that are trained on massive sets of data to build general language capabilities. And these pre trained models like bert, like GPT, give developers a head start versus training models from scratch. But then incomes fine tuning, which allows us to take these raw models, these foundation models, and fine tune them for our specific use cases. So let's think about an example. Let's say you want to fine tune a model to be able to take pizza orders, to be able to have conversations, answer questions about pizza, and finally be able to allow customers to buy pizza. You can take a pre existing set of conversations that exemplify the back and forth between a pizza shop and a customer, load that in, fine tune a model, and then all of a sudden that model is going to be be much better at having conversations about pizza ordering. The model updates the weights to be better at understanding certain pizza terminology, questions, responses, tone, everything. And fine tuning is much faster than a full training and it produces much higher accuracy. And fine tuning allows pre trained models to be fine tuned for real world use cases. And finally you can take a single foundational model and fine tune it it any number of times for any number of use cases. And there are a lot of great services out there that allow you to do that. And again, it's all about the quality of your data. So if you have a really good data set that you're going to fine tune a model on, the model is going to be really, really good. And conversely, if you have a poor quality data set, it's not going to perform as well. All right, let me pause for a second and talk about AI Camp. So as mentioned earlier, this video, all of its content, the animations have been created in collaboration with students from AI Camp. Camp. AI Camp is a learning experience for students that are age 13 and above. You work in small personalized groups with experienced mentors. You work together to create an AI product using nlp, Computer vision and Data Science. AI Camp has both a three week and a one week program during summer that requires zero programming experience. And they also have a new program which is 10 weeks long during the school year, which is less intensive than the one week and three week programs for those students who are really busy. Busy AI Camp's mission is to provide students with deep knowledge in artificial intelligence which will position them to be ready for AI in the real world. I'll link an article from USA Today in the description all about AI Camp. But if you're a student or if you're a parent of a student within this age, I would highly Recommend Checking out AI camp. Go to AI-camp.org to learn more. Now let's talk about limitations and challenges of large language models. As capable as LLMs are they, they still have a lot of limitations. Recent models continue to get better, but they are still flawed. They're incredibly valuable and knowledgeable in certain ways, but they're also deeply flawed in others like math and logic and reasoning. They still struggle a lot of the time versus humans which understand concepts like that pretty easily. Also, bias and safety continue to be a big problem. Large language models are trained on data created by humans, which is naturally flawed. Humans have opinions on everything and those opinions trickle down into these models. These data sets may include harmful or biased information, and some companies take their models a step further and provide a level of censorship to those models. And that's an entire discussion in itself whether censorship is worthwhile or not. I know a lot of you already know my opinions on this from my previous videos. And another big limitation of LLMs historically has been that they only have knowledge up until the point where their training occurred. But but that is starting to be solved with ChatGPT being able to browse the web for example Grok from X, AI being able to access live tweets. But there's still a lot of kinks to be worked out with this. Also, another big challenge for large language models is hallucinations, which means that they sometimes just make things up or get things patently wrong. And they will be so confident in being wrong too. They will state things with the utmost confidence, but will be completely wrong. Look at this example. How many letters are in the string? And then we give it a random string of characters and then the answer is the string has 16 letters even though it only has 15 letters. Another problem is that large language models are extremely hardware intensive. They cost a ton to train and to fine tune because it takes so much processing power to do that. And there's a lot of ethics to consider too. A lot of AI companies say they aren't training their models on copyrighted material, but that has been been found to be false. Currently there are A ton of lawsuits going through the courts about this issue. Next, let's talk about the real world applications of large language models. Why are they so valuable, why are they so talked about? And why are they transforming the world right in front of our eyes? Large language models can be used for a wide variety of tasks, not just chatbots. They can be used for language translation, they can be used for coding, they can be used as programming assistants, they can be used for summarization, question answering, essay writing, translation, and even image and video creation. Basically any type of thought problem that a human can do with a computer. Large language models can likely also do, if not today, pretty soon, in the future. Now let's talk about current advancements and research. Currently there's a lot of talk about knowledge distillation, which basically means transferring key knowledge from very large cutting edge models to smaller, more efficient models models. Think about it like a professor condensing decades of experience in a textbook down to something that the students can comprehend. And this allows smaller language models to benefit from the knowledge gained from these large language models, but still run highly efficiently on everyday consumer hardware. And it makes large language models more accessible and practical to run, even on cell phones or other end devices. There's also been a lot of research and emphasis on rag, which is retrieval, augmented generation, which basically means you're giving large language models ability to look up information outside of the data that it was trained on. You're using vector databases the same way that large language models are trained, but you're able to store massive amounts of additional data that can be queried by that large language model. Now, let's talk about the ethical considerations and there's a lot to think about here, and I'm just touching on some of the major topics. First, we already talked about that the models are trained on potentially copyrighted material. And if that's the case, is that fair use? Probably not. Next, these models can and will be used for harmful acts. There's no avoiding it. Large language models can be used to scam other people, to create massive misinformation and disinformation campaigns, including fake images, fake text, fake opinions, and almost definitely the entire white collar workforce is going to be disrupted by large language models. As I mentioned, anything anybody can do in front of a computer is probably something that the AI can also do. Do so lawyers, writers, programmers, there are so many different professions that are going to be completely disrupted by artificial intelligence. And then finally, AGI, what happens when AI becomes so smart and maybe even starts thinking for itself? This is where we have to have something called alignment, which means the AI is aligned to the same incentives and outcomes as humans. So last, let's talk about what's happening on the cutting edge and in the immediate future. There are a number of ways large language models can be improved. First, they can fact check themselves with information gathered from the web, but obviously you can see the inherent flaws in that. Then we also touched on mixture of experts, which is an incredible new technology which allows multiple models to kind of be merged together, all fine tuned to be experts in certain domains. And then when the actual prompt comes through, it chooses which of those experts to use. So these are huge models that actually run really, really efficiently. Then there's a lot of work on multimodality. So taking input from voice, from images, from video, every possible input source and having a single output from that. There's also a lot of work being done to improve reasoning ability. Having models think slowly is a new trend that I've been seeing in papers like Orca 2, which basically just forces a large language model to think about problems step by step rather than trying to jump to the final conclusion immediately. And then also larger context sizes. If you want a large language model to process a huge amount of data, it has to have a very large context window. And a context window is just how much information you can give to a prompt to get the output. And one way to achieve that is by giving large language models memory. With projects like MEM GPT, which I did a video on, and I'll drop that in the description below and that just means giving models external memory from that core data set that they were trained on. So that's it for today. If you liked this video, please consider giving a like and subscribe. Check out AI Camp. I'll drop all the information in the description below and of course check out any of my other AI videos if you want to learn even more. I'll see you in the next one.