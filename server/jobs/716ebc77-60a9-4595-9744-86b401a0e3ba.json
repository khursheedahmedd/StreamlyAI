{"id": "716ebc77-60a9-4595-9744-86b401a0e3ba", "status": "completed", "progress": 100, "video_url": "https://youtu.be/0z9_MhcYvcY?si=1RFODGBoa1QkgbXh", "title": "What is Agentic RAG?", "thumbnail": "https://i.ytimg.com/vi/0z9_MhcYvcY/sddefault.jpg", "duration": 341, "created_at": 1751720677.2417111, "transcription": "So we all know what retrieval augmented generation is, but let's just do a quick refresher. Retrieval augmented generation is a powerful and popular pipeline that enhances responses from a large language model. It does this by incorporating relevant data retrieved from a vector database, adding it as context to the prompt, and sending it to the LLM for generation. What this does is it allows the LLM to ground its response in concrete and accurate information, and that improves the quality and reliability of the response. Let me quickly sketch it out. So let's say we have a user or an application even, and they send a query. Now, without retrieval augmented generation, this query is going to go and get itself interpolated into a prompt, and from there that's going to hit the LLM and that's going to generate an output to make this rag, we can add a vector database. So instead of just going directly and getting itself interpolated into the prompt, it's going to hit this vector db. And the response from that vector DB is going to be used as context for the prompt. Now, in this typical RAG pipeline, we call the LLM only once, and we use it solely to generate a response. But what if we could leverage the LLM not just for responses, but also for additional tasks, like deciding which vector database to query if we have multiple databases, or even determining the type of response to give should it answer with text, generate a chart, or even provide a code snippet? And that would all be dependent on the context of that query. So this is where the agentic RAG pipeline comes into play. In agentic rag, we use the LLM as an agent, and the LLM goes beyond just generating a response. It takes on an active role and can make decisions that will improve both the relevance and accuracy of the retrieved data. Now let's explore how we can augment the initial process with an agent and a couple of different sources of data. So instead of just one single source, let's add a second. And the first one can be internal documentation, right? And the second one can be general industry knowledge. Now, in the internal documentation, we're going to have things like policies, procedures and guidelines. And the general knowledge base will have things like industry standards, best practices and public resources. So how can we get the LLM to use the vector database that contains the data that will be most relevant to the query? Let's add that agent into this pipeline. Now, this agent can intelligently decide which database to query based on the user's question. And the agent isn't making a random guess, it's leveraging the LLM's language understanding capabilities to interpret the query and determine its context. So if an employee asks what's the company's policy on remote work during the holidays? It would route that to the internal documentation and that response will be used as context for the prompt. But if the question is more general, like what are the industry standards for remote work and tech companies? The agent's going to route that to the general knowledge database and that context is going to be used within that prompt. Powered by an LLM and properly trained, the agent analyzes the query and based on the understanding of the content and the context, decides which database to use. But they're not always going to ask questions that are generally or genuinely relevant to any of the stuff that we have in our vector db. So what if someone asks a question that is just totally out of left field, like who won the world series in 2015? What the agent can do at that point is it could route it to a fail safe. So because the agent is able to recognize the context of the query, it could recognize that it's not a part of the two databases that we have get routed to the fail safe and return back, Sorry, I don't have the information you're looking for. This agentic RAG pipeline can be used in customer support systems and legal tech. For example, a lawyer can source answers to their questions from like their internal briefs and then in another query just get stuff from public caseload databases. The agent can be utilized in a ton of ways. Agentic rag is an evolution in how we enhance the rag pipeline by moving beyond simple response generation to more intelligent decision making. By allowing an agent to choose the best data sources and potentially even incorporate external information like real time data or third party services, we can create a pipeline that's more responsive, more accurate and more adaptable. This approach opens up so many possibilities for applications in customer service, legal, tech, healthcare, virtually any field. As the technology continues to evolve, we will see AI systems that truly understand context and can deliver amazing values to the end user. SA.", "timestamps": [{"start_time": 1.12, "end_time": 1.2, "text": "So"}, {"start_time": 1.2, "end_time": 1.32, "text": "we"}, {"start_time": 1.32, "end_time": 1.56, "text": "all"}, {"start_time": 1.56, "end_time": 1.92, "text": "know"}, {"start_time": 2.24, "end_time": 2.56, "text": "what"}, {"start_time": 2.56, "end_time": 3.2, "text": "retrieval"}, {"start_time": 3.2, "end_time": 3.8, "text": "augmented"}, {"start_time": 3.8, "end_time": 4.28, "text": "generation"}, {"start_time": 4.28, "end_time": 4.64, "text": "is,"}, {"start_time": 4.72, "end_time": 5.0, "text": "but"}, {"start_time": 5.0, "end_time": 5.24, "text": "let's"}, {"start_time": 5.24, "end_time": 5.36, "text": "just"}, {"start_time": 5.36, "end_time": 5.48, "text": "do"}, {"start_time": 5.48, "end_time": 5.6, "text": "a"}, {"start_time": 5.6, "end_time": 5.76, "text": "quick"}, {"start_time": 5.76, "end_time": 6.4, "text": "refresher."}, {"start_time": 6.88, "end_time": 7.52, "text": "Retrieval"}, {"start_time": 7.52, "end_time": 8.0, "text": "augmented"}, {"start_time": 8.0, "end_time": 8.56, "text": "generation"}, {"start_time": 8.64, "end_time": 8.92, "text": "is"}, {"start_time": 8.92, "end_time": 9.12, "text": "a"}, {"start_time": 9.12, "end_time": 9.6, "text": "powerful"}, {"start_time": 9.6, "end_time": 9.92, "text": "and"}, {"start_time": 9.92, "end_time": 10.56, "text": "popular"}, {"start_time": 10.64, "end_time": 11.24, "text": "pipeline"}, {"start_time": 11.24, "end_time": 11.44, "text": "that"}, {"start_time": 11.44, "end_time": 11.92, "text": "enhances"}, {"start_time": 11.92, "end_time": 12.56, "text": "responses"}, {"start_time": 12.56, "end_time": 12.76, "text": "from"}, {"start_time": 12.76, "end_time": 12.92, "text": "a"}, {"start_time": 12.92, "end_time": 13.12, "text": "large"}, {"start_time": 13.12, "end_time": 13.56, "text": "language"}, {"start_time": 13.56, "end_time": 14.0, "text": "model."}, {"start_time": 14.16, "end_time": 14.44, "text": "It"}, {"start_time": 14.44, "end_time": 14.6, "text": "does"}, {"start_time": 14.6, "end_time": 14.8, "text": "this"}, {"start_time": 14.8, "end_time": 15.0, "text": "by"}, {"start_time": 15.0, "end_time": 15.6, "text": "incorporating"}, {"start_time": 15.76, "end_time": 16.36, "text": "relevant"}, {"start_time": 16.36, "end_time": 16.64, "text": "data"}, {"start_time": 16.64, "end_time": 17.16, "text": "retrieved"}, {"start_time": 17.16, "end_time": 17.32, "text": "from"}, {"start_time": 17.32, "end_time": 17.44, "text": "a"}, {"start_time": 17.44, "end_time": 17.72, "text": "vector"}, {"start_time": 17.72, "end_time": 18.32, "text": "database,"}, {"start_time": 18.4, "end_time": 18.76, "text": "adding"}, {"start_time": 18.76, "end_time": 18.92, "text": "it"}, {"start_time": 18.92, "end_time": 19.12, "text": "as"}, {"start_time": 19.12, "end_time": 19.56, "text": "context"}, {"start_time": 19.56, "end_time": 19.76, "text": "to"}, {"start_time": 19.76, "end_time": 19.88, "text": "the"}, {"start_time": 19.88, "end_time": 20.16, "text": "prompt,"}, {"start_time": 20.16, "end_time": 20.44, "text": "and"}, {"start_time": 20.44, "end_time": 20.8, "text": "sending"}, {"start_time": 20.8, "end_time": 20.88, "text": "it"}, {"start_time": 20.88, "end_time": 20.96, "text": "to"}, {"start_time": 20.96, "end_time": 21.08, "text": "the"}, {"start_time": 21.08, "end_time": 21.56, "text": "LLM"}, {"start_time": 21.56, "end_time": 21.72, "text": "for"}, {"start_time": 21.72, "end_time": 22.24, "text": "generation."}, {"start_time": 22.72, "end_time": 23.0, "text": "What"}, {"start_time": 23.0, "end_time": 23.28, "text": "this"}, {"start_time": 23.28, "end_time": 23.6, "text": "does"}, {"start_time": 23.6, "end_time": 23.8, "text": "is"}, {"start_time": 23.8, "end_time": 24.04, "text": "it"}, {"start_time": 24.04, "end_time": 24.4, "text": "allows"}, {"start_time": 24.4, "end_time": 24.52, "text": "the"}, {"start_time": 24.52, "end_time": 25.0, "text": "LLM"}, {"start_time": 25.0, "end_time": 25.16, "text": "to"}, {"start_time": 25.16, "end_time": 25.36, "text": "ground"}, {"start_time": 25.36, "end_time": 25.6, "text": "its"}, {"start_time": 25.6, "end_time": 26.08, "text": "response"}, {"start_time": 26.08, "end_time": 26.32, "text": "in"}, {"start_time": 26.32, "end_time": 26.8, "text": "concrete"}, {"start_time": 26.8, "end_time": 27.04, "text": "and"}, {"start_time": 27.04, "end_time": 27.48, "text": "accurate"}, {"start_time": 27.48, "end_time": 27.76, "text": "information,"}, {"start_time": 28.0, "end_time": 28.28, "text": "and"}, {"start_time": 28.28, "end_time": 28.48, "text": "that"}, {"start_time": 28.48, "end_time": 28.88, "text": "improves"}, {"start_time": 28.88, "end_time": 29.04, "text": "the"}, {"start_time": 29.04, "end_time": 29.52, "text": "quality"}, {"start_time": 29.93, "end_time": 30.09, "text": "and"}, {"start_time": 30.09, "end_time": 30.65, "text": "reliability"}, {"start_time": 30.65, "end_time": 30.85, "text": "of"}, {"start_time": 30.85, "end_time": 31.01, "text": "the"}, {"start_time": 31.01, "end_time": 31.53, "text": "response."}, {"start_time": 31.69, "end_time": 31.97, "text": "Let"}, {"start_time": 31.97, "end_time": 32.13, "text": "me"}, {"start_time": 32.13, "end_time": 32.37, "text": "quickly"}, {"start_time": 32.37, "end_time": 32.69, "text": "sketch"}, {"start_time": 32.69, "end_time": 32.81, "text": "it"}, {"start_time": 32.81, "end_time": 33.05, "text": "out."}, {"start_time": 34.41, "end_time": 34.65, "text": "So"}, {"start_time": 34.65, "end_time": 34.81, "text": "let's"}, {"start_time": 34.81, "end_time": 34.93, "text": "say"}, {"start_time": 34.93, "end_time": 35.09, "text": "we"}, {"start_time": 35.09, "end_time": 35.25, "text": "have"}, {"start_time": 35.25, "end_time": 35.41, "text": "a"}, {"start_time": 35.41, "end_time": 35.93, "text": "user"}, {"start_time": 37.13, "end_time": 37.45, "text": "or"}, {"start_time": 37.45, "end_time": 37.65, "text": "an"}, {"start_time": 37.65, "end_time": 38.21, "text": "application"}, {"start_time": 38.21, "end_time": 38.57, "text": "even,"}, {"start_time": 41.21, "end_time": 41.49, "text": "and"}, {"start_time": 41.49, "end_time": 41.65, "text": "they"}, {"start_time": 41.65, "end_time": 41.89, "text": "send"}, {"start_time": 41.89, "end_time": 42.01, "text": "a"}, {"start_time": 42.01, "end_time": 42.33, "text": "query."}, {"start_time": 44.57, "end_time": 44.85, "text": "Now,"}, {"start_time": 44.85, "end_time": 45.13, "text": "without"}, {"start_time": 45.37, "end_time": 46.049, "text": "retrieval"}, {"start_time": 46.049, "end_time": 46.53, "text": "augmented"}, {"start_time": 46.53, "end_time": 47.05, "text": "generation,"}, {"start_time": 47.13, "end_time": 47.45, "text": "this"}, {"start_time": 47.45, "end_time": 47.81, "text": "query"}, {"start_time": 47.81, "end_time": 48.05, "text": "is"}, {"start_time": 48.05, "end_time": 48.21, "text": "going"}, {"start_time": 48.21, "end_time": 48.41, "text": "to"}, {"start_time": 48.41, "end_time": 48.73, "text": "go"}, {"start_time": 49.29, "end_time": 49.57, "text": "and"}, {"start_time": 49.57, "end_time": 49.73, "text": "get"}, {"start_time": 49.73, "end_time": 50.13, "text": "itself"}, {"start_time": 50.13, "end_time": 50.97, "text": "interpolated"}, {"start_time": 50.97, "end_time": 51.33, "text": "into"}, {"start_time": 51.33, "end_time": 51.57, "text": "a"}, {"start_time": 51.57, "end_time": 51.93, "text": "prompt,"}, {"start_time": 55.46, "end_time": 55.58, "text": "and"}, {"start_time": 55.58, "end_time": 55.74, "text": "from"}, {"start_time": 55.74, "end_time": 56.02, "text": "there"}, {"start_time": 56.98, "end_time": 57.38, "text": "that's"}, {"start_time": 57.38, "end_time": 57.46, "text": "going"}, {"start_time": 57.46, "end_time": 57.58, "text": "to"}, {"start_time": 57.58, "end_time": 57.82, "text": "hit"}, {"start_time": 57.82, "end_time": 58.06, "text": "the"}, {"start_time": 58.06, "end_time": 58.66, "text": "LLM"}, {"start_time": 61.3, "end_time": 61.58, "text": "and"}, {"start_time": 61.58, "end_time": 61.86, "text": "that's"}, {"start_time": 61.86, "end_time": 62.02, "text": "going"}, {"start_time": 62.02, "end_time": 62.34, "text": "to"}, {"start_time": 62.82, "end_time": 63.38, "text": "generate"}, {"start_time": 63.38, "end_time": 63.58, "text": "an"}, {"start_time": 63.58, "end_time": 64.1, "text": "output"}, {"start_time": 67.94, "end_time": 68.22, "text": "to"}, {"start_time": 68.22, "end_time": 68.38, "text": "make"}, {"start_time": 68.38, "end_time": 68.62, "text": "this"}, {"start_time": 68.62, "end_time": 69.14, "text": "rag,"}, {"start_time": 69.14, "end_time": 69.42, "text": "we"}, {"start_time": 69.42, "end_time": 69.62, "text": "can"}, {"start_time": 69.62, "end_time": 69.94, "text": "add"}, {"start_time": 70.18, "end_time": 70.46, "text": "a"}, {"start_time": 70.46, "end_time": 70.86, "text": "vector"}, {"start_time": 70.86, "end_time": 71.46, "text": "database."}, {"start_time": 72.58, "end_time": 72.86, "text": "So"}, {"start_time": 72.86, "end_time": 73.1, "text": "instead"}, {"start_time": 73.1, "end_time": 73.22, "text": "of"}, {"start_time": 73.22, "end_time": 73.34, "text": "just"}, {"start_time": 73.34, "end_time": 73.54, "text": "going"}, {"start_time": 73.54, "end_time": 73.9, "text": "directly"}, {"start_time": 73.9, "end_time": 74.06, "text": "and"}, {"start_time": 74.06, "end_time": 74.22, "text": "getting"}, {"start_time": 74.22, "end_time": 74.5, "text": "itself"}, {"start_time": 74.5, "end_time": 75.02, "text": "interpolated"}, {"start_time": 75.02, "end_time": 75.22, "text": "into"}, {"start_time": 75.22, "end_time": 75.38, "text": "the"}, {"start_time": 75.38, "end_time": 75.62, "text": "prompt,"}, {"start_time": 75.62, "end_time": 75.86, "text": "it's"}, {"start_time": 75.86, "end_time": 75.94, "text": "going"}, {"start_time": 75.94, "end_time": 76.02, "text": "to"}, {"start_time": 76.02, "end_time": 76.22, "text": "hit"}, {"start_time": 76.22, "end_time": 76.5, "text": "this"}, {"start_time": 76.5, "end_time": 76.9, "text": "vector"}, {"start_time": 76.9, "end_time": 77.38, "text": "db."}, {"start_time": 77.54, "end_time": 77.86, "text": "And"}, {"start_time": 77.86, "end_time": 78.06, "text": "the"}, {"start_time": 78.06, "end_time": 78.46, "text": "response"}, {"start_time": 78.46, "end_time": 78.62, "text": "from"}, {"start_time": 78.62, "end_time": 78.78, "text": "that"}, {"start_time": 78.78, "end_time": 79.02, "text": "vector"}, {"start_time": 79.02, "end_time": 79.34, "text": "DB"}, {"start_time": 79.34, "end_time": 79.54, "text": "is"}, {"start_time": 79.54, "end_time": 79.62, "text": "going"}, {"start_time": 79.62, "end_time": 79.7, "text": "to"}, {"start_time": 79.7, "end_time": 79.82, "text": "be"}, {"start_time": 79.82, "end_time": 80.1, "text": "used"}, {"start_time": 80.26, "end_time": 80.62, "text": "as"}, {"start_time": 80.62, "end_time": 81.22, "text": "context"}, {"start_time": 81.22, "end_time": 81.46, "text": "for"}, {"start_time": 81.46, "end_time": 81.58, "text": "the"}, {"start_time": 81.58, "end_time": 81.94, "text": "prompt."}, {"start_time": 83.36, "end_time": 83.48, "text": "Now,"}, {"start_time": 83.48, "end_time": 83.64, "text": "in"}, {"start_time": 83.64, "end_time": 83.88, "text": "this"}, {"start_time": 83.88, "end_time": 84.36, "text": "typical"}, {"start_time": 84.36, "end_time": 84.76, "text": "RAG"}, {"start_time": 84.76, "end_time": 85.28, "text": "pipeline,"}, {"start_time": 85.28, "end_time": 85.4, "text": "we"}, {"start_time": 85.4, "end_time": 85.6, "text": "call"}, {"start_time": 85.6, "end_time": 85.8, "text": "the"}, {"start_time": 85.8, "end_time": 86.4, "text": "LLM"}, {"start_time": 86.4, "end_time": 86.8, "text": "only"}, {"start_time": 86.8, "end_time": 87.2, "text": "once,"}, {"start_time": 87.28, "end_time": 87.56, "text": "and"}, {"start_time": 87.56, "end_time": 87.72, "text": "we"}, {"start_time": 87.72, "end_time": 87.88, "text": "use"}, {"start_time": 87.88, "end_time": 88.16, "text": "it"}, {"start_time": 88.16, "end_time": 88.64, "text": "solely"}, {"start_time": 88.64, "end_time": 88.96, "text": "to"}, {"start_time": 88.96, "end_time": 89.36, "text": "generate"}, {"start_time": 89.36, "end_time": 89.56, "text": "a"}, {"start_time": 89.56, "end_time": 90.08, "text": "response."}, {"start_time": 90.8, "end_time": 91.12, "text": "But"}, {"start_time": 91.12, "end_time": 91.32, "text": "what"}, {"start_time": 91.32, "end_time": 91.48, "text": "if"}, {"start_time": 91.48, "end_time": 91.64, "text": "we"}, {"start_time": 91.64, "end_time": 91.84, "text": "could"}, {"start_time": 91.84, "end_time": 92.24, "text": "leverage"}, {"start_time": 92.24, "end_time": 92.44, "text": "the"}, {"start_time": 92.44, "end_time": 93.04, "text": "LLM"}, {"start_time": 93.2, "end_time": 93.52, "text": "not"}, {"start_time": 93.52, "end_time": 93.76, "text": "just"}, {"start_time": 93.76, "end_time": 94.0, "text": "for"}, {"start_time": 94.0, "end_time": 94.72, "text": "responses,"}, {"start_time": 94.88, "end_time": 95.2, "text": "but"}, {"start_time": 95.2, "end_time": 95.48, "text": "also"}, {"start_time": 95.48, "end_time": 95.72, "text": "for"}, {"start_time": 95.72, "end_time": 96.24, "text": "additional"}, {"start_time": 96.24, "end_time": 96.72, "text": "tasks,"}, {"start_time": 96.72, "end_time": 97.12, "text": "like"}, {"start_time": 97.36, "end_time": 97.92, "text": "deciding"}, {"start_time": 97.92, "end_time": 98.199, "text": "which"}, {"start_time": 98.199, "end_time": 98.6, "text": "vector"}, {"start_time": 98.6, "end_time": 99.0, "text": "database"}, {"start_time": 99.0, "end_time": 99.16, "text": "to"}, {"start_time": 99.16, "end_time": 99.44, "text": "query"}, {"start_time": 99.44, "end_time": 99.6, "text": "if"}, {"start_time": 99.6, "end_time": 99.72, "text": "we"}, {"start_time": 99.72, "end_time": 99.88, "text": "have"}, {"start_time": 99.88, "end_time": 100.24, "text": "multiple"}, {"start_time": 100.24, "end_time": 100.88, "text": "databases,"}, {"start_time": 100.96, "end_time": 101.24, "text": "or"}, {"start_time": 101.24, "end_time": 101.44, "text": "even"}, {"start_time": 101.44, "end_time": 101.96, "text": "determining"}, {"start_time": 101.96, "end_time": 102.12, "text": "the"}, {"start_time": 102.12, "end_time": 102.36, "text": "type"}, {"start_time": 102.36, "end_time": 102.52, "text": "of"}, {"start_time": 102.52, "end_time": 102.96, "text": "response"}, {"start_time": 102.96, "end_time": 103.16, "text": "to"}, {"start_time": 103.16, "end_time": 103.44, "text": "give"}, {"start_time": 103.76, "end_time": 104.08, "text": "should"}, {"start_time": 104.08, "end_time": 104.28, "text": "it"}, {"start_time": 104.28, "end_time": 104.6, "text": "answer"}, {"start_time": 104.6, "end_time": 104.88, "text": "with"}, {"start_time": 104.88, "end_time": 105.36, "text": "text,"}, {"start_time": 105.36, "end_time": 105.88, "text": "generate"}, {"start_time": 105.88, "end_time": 106.08, "text": "a"}, {"start_time": 106.08, "end_time": 106.36, "text": "chart,"}, {"start_time": 106.36, "end_time": 106.52, "text": "or"}, {"start_time": 106.52, "end_time": 106.72, "text": "even"}, {"start_time": 106.72, "end_time": 107.0, "text": "provide"}, {"start_time": 107.0, "end_time": 107.2, "text": "a"}, {"start_time": 107.2, "end_time": 107.4, "text": "code"}, {"start_time": 107.4, "end_time": 107.92, "text": "snippet?"}, {"start_time": 108.07, "end_time": 108.19, "text": "And"}, {"start_time": 108.19, "end_time": 108.35, "text": "that"}, {"start_time": 108.35, "end_time": 108.55, "text": "would"}, {"start_time": 108.55, "end_time": 108.79, "text": "all"}, {"start_time": 108.79, "end_time": 108.99, "text": "be"}, {"start_time": 108.99, "end_time": 109.63, "text": "dependent"}, {"start_time": 109.63, "end_time": 109.87, "text": "on"}, {"start_time": 109.87, "end_time": 110.03, "text": "the"}, {"start_time": 110.03, "end_time": 110.39, "text": "context"}, {"start_time": 110.39, "end_time": 110.59, "text": "of"}, {"start_time": 110.59, "end_time": 110.75, "text": "that"}, {"start_time": 110.75, "end_time": 111.11, "text": "query."}, {"start_time": 112.31, "end_time": 112.59, "text": "So"}, {"start_time": 112.59, "end_time": 112.75, "text": "this"}, {"start_time": 112.75, "end_time": 112.95, "text": "is"}, {"start_time": 112.95, "end_time": 113.23, "text": "where"}, {"start_time": 113.23, "end_time": 113.51, "text": "the"}, {"start_time": 113.51, "end_time": 114.31, "text": "agentic"}, {"start_time": 115.03, "end_time": 115.55, "text": "RAG"}, {"start_time": 115.55, "end_time": 116.23, "text": "pipeline"}, {"start_time": 117.91, "end_time": 118.23, "text": "comes"}, {"start_time": 118.23, "end_time": 118.51, "text": "into"}, {"start_time": 118.51, "end_time": 118.87, "text": "play."}, {"start_time": 118.87, "end_time": 119.19, "text": "In"}, {"start_time": 119.19, "end_time": 119.79, "text": "agentic"}, {"start_time": 119.79, "end_time": 120.07, "text": "rag,"}, {"start_time": 120.07, "end_time": 120.27, "text": "we"}, {"start_time": 120.27, "end_time": 120.43, "text": "use"}, {"start_time": 120.43, "end_time": 120.59, "text": "the"}, {"start_time": 120.59, "end_time": 121.27, "text": "LLM"}, {"start_time": 121.75, "end_time": 122.03, "text": "as"}, {"start_time": 122.03, "end_time": 122.19, "text": "an"}, {"start_time": 122.19, "end_time": 122.59, "text": "agent,"}, {"start_time": 122.59, "end_time": 122.79, "text": "and"}, {"start_time": 122.79, "end_time": 122.91, "text": "the"}, {"start_time": 122.91, "end_time": 123.39, "text": "LLM"}, {"start_time": 123.39, "end_time": 123.59, "text": "goes"}, {"start_time": 123.59, "end_time": 123.91, "text": "beyond"}, {"start_time": 123.91, "end_time": 124.189, "text": "just"}, {"start_time": 124.189, "end_time": 124.63, "text": "generating"}, {"start_time": 124.63, "end_time": 124.83, "text": "a"}, {"start_time": 124.83, "end_time": 125.35, "text": "response."}, {"start_time": 125.51, "end_time": 125.79, "text": "It"}, {"start_time": 125.79, "end_time": 126.11, "text": "takes"}, {"start_time": 126.11, "end_time": 126.31, "text": "on"}, {"start_time": 126.31, "end_time": 126.55, "text": "an"}, {"start_time": 126.55, "end_time": 126.95, "text": "active"}, {"start_time": 126.95, "end_time": 127.27, "text": "role"}, {"start_time": 127.27, "end_time": 127.55, "text": "and"}, {"start_time": 127.55, "end_time": 127.75, "text": "can"}, {"start_time": 127.75, "end_time": 127.95, "text": "make"}, {"start_time": 127.95, "end_time": 128.43, "text": "decisions"}, {"start_time": 128.43, "end_time": 128.59, "text": "that"}, {"start_time": 128.59, "end_time": 128.79, "text": "will"}, {"start_time": 128.79, "end_time": 129.11, "text": "improve"}, {"start_time": 129.27, "end_time": 129.67, "text": "both"}, {"start_time": 129.99, "end_time": 130.31, "text": "the"}, {"start_time": 130.31, "end_time": 130.83, "text": "relevance"}, {"start_time": 130.83, "end_time": 131.19, "text": "and"}, {"start_time": 131.19, "end_time": 131.71, "text": "accuracy"}, {"start_time": 131.71, "end_time": 131.91, "text": "of"}, {"start_time": 131.91, "end_time": 132.11, "text": "the"}, {"start_time": 132.11, "end_time": 132.55, "text": "retrieved"}, {"start_time": 132.55, "end_time": 132.87, "text": "data."}, {"start_time": 133.35, "end_time": 133.63, "text": "Now"}, {"start_time": 133.63, "end_time": 133.87, "text": "let's"}, {"start_time": 133.87, "end_time": 134.35, "text": "explore"}, {"start_time": 134.35, "end_time": 134.59, "text": "how"}, {"start_time": 134.59, "end_time": 134.75, "text": "we"}, {"start_time": 134.75, "end_time": 134.91, "text": "can"}, {"start_time": 134.91, "end_time": 135.27, "text": "augment"}, {"start_time": 135.27, "end_time": 135.47, "text": "the"}, {"start_time": 135.47, "end_time": 135.95, "text": "initial"}, {"start_time": 135.95, "end_time": 136.31, "text": "process"}, {"start_time": 136.88, "end_time": 137.0, "text": "with"}, {"start_time": 137.0, "end_time": 137.16, "text": "an"}, {"start_time": 137.16, "end_time": 137.56, "text": "agent"}, {"start_time": 137.56, "end_time": 137.8, "text": "and"}, {"start_time": 137.8, "end_time": 137.92, "text": "a"}, {"start_time": 137.92, "end_time": 138.12, "text": "couple"}, {"start_time": 138.12, "end_time": 138.28, "text": "of"}, {"start_time": 138.28, "end_time": 138.56, "text": "different"}, {"start_time": 138.56, "end_time": 138.96, "text": "sources"}, {"start_time": 138.96, "end_time": 139.16, "text": "of"}, {"start_time": 139.16, "end_time": 139.44, "text": "data."}, {"start_time": 140.32, "end_time": 140.6, "text": "So"}, {"start_time": 140.6, "end_time": 140.84, "text": "instead"}, {"start_time": 140.84, "end_time": 141.0, "text": "of"}, {"start_time": 141.0, "end_time": 141.24, "text": "just"}, {"start_time": 141.24, "end_time": 141.52, "text": "one"}, {"start_time": 141.52, "end_time": 141.88, "text": "single"}, {"start_time": 141.88, "end_time": 142.4, "text": "source,"}, {"start_time": 142.4, "end_time": 142.76, "text": "let's"}, {"start_time": 142.76, "end_time": 142.96, "text": "add"}, {"start_time": 142.96, "end_time": 143.16, "text": "a"}, {"start_time": 143.16, "end_time": 143.44, "text": "second."}, {"start_time": 145.68, "end_time": 146.0, "text": "And"}, {"start_time": 146.0, "end_time": 146.2, "text": "the"}, {"start_time": 146.2, "end_time": 146.4, "text": "first"}, {"start_time": 146.4, "end_time": 146.64, "text": "one"}, {"start_time": 146.64, "end_time": 146.84, "text": "can"}, {"start_time": 146.84, "end_time": 147.12, "text": "be"}, {"start_time": 148.8, "end_time": 149.6, "text": "internal"}, {"start_time": 149.92, "end_time": 150.72, "text": "documentation,"}, {"start_time": 150.72, "end_time": 151.12, "text": "right?"}, {"start_time": 151.6, "end_time": 151.88, "text": "And"}, {"start_time": 151.88, "end_time": 152.04, "text": "the"}, {"start_time": 152.04, "end_time": 152.28, "text": "second"}, {"start_time": 152.28, "end_time": 152.64, "text": "one"}, {"start_time": 152.96, "end_time": 153.24, "text": "can"}, {"start_time": 153.24, "end_time": 153.44, "text": "be"}, {"start_time": 153.44, "end_time": 153.76, "text": "general"}, {"start_time": 153.92, "end_time": 154.32, "text": "industry"}, {"start_time": 154.32, "end_time": 154.96, "text": "knowledge."}, {"start_time": 159.84, "end_time": 160.24, "text": "Now,"}, {"start_time": 160.24, "end_time": 160.48, "text": "in"}, {"start_time": 160.48, "end_time": 160.6, "text": "the"}, {"start_time": 160.6, "end_time": 161.04, "text": "internal"}, {"start_time": 161.04, "end_time": 161.56, "text": "documentation,"}, {"start_time": 161.56, "end_time": 161.76, "text": "we're"}, {"start_time": 161.76, "end_time": 161.88, "text": "going"}, {"start_time": 161.88, "end_time": 162.0, "text": "to"}, {"start_time": 162.0, "end_time": 162.12, "text": "have"}, {"start_time": 162.12, "end_time": 162.32, "text": "things"}, {"start_time": 162.32, "end_time": 162.56, "text": "like"}, {"start_time": 162.56, "end_time": 163.2, "text": "policies,"}, {"start_time": 163.2, "end_time": 163.8, "text": "procedures"}, {"start_time": 163.8, "end_time": 163.96, "text": "and"}, {"start_time": 163.96, "end_time": 164.44, "text": "guidelines."}, {"start_time": 164.44, "end_time": 164.64, "text": "And"}, {"start_time": 164.64, "end_time": 164.84, "text": "the"}, {"start_time": 164.84, "end_time": 165.12, "text": "general"}, {"start_time": 165.12, "end_time": 165.52, "text": "knowledge"}, {"start_time": 165.52, "end_time": 165.92, "text": "base"}, {"start_time": 166.22, "end_time": 166.38, "text": "will"}, {"start_time": 166.38, "end_time": 166.58, "text": "have"}, {"start_time": 166.58, "end_time": 166.78, "text": "things"}, {"start_time": 166.78, "end_time": 167.02, "text": "like"}, {"start_time": 167.02, "end_time": 167.34, "text": "industry"}, {"start_time": 167.42, "end_time": 168.06, "text": "standards,"}, {"start_time": 168.22, "end_time": 168.5, "text": "best"}, {"start_time": 168.5, "end_time": 169.1, "text": "practices"}, {"start_time": 169.18, "end_time": 169.46, "text": "and"}, {"start_time": 169.46, "end_time": 169.7, "text": "public"}, {"start_time": 169.7, "end_time": 170.38, "text": "resources."}, {"start_time": 171.5, "end_time": 171.74, "text": "So"}, {"start_time": 171.74, "end_time": 171.86, "text": "how"}, {"start_time": 171.86, "end_time": 172.02, "text": "can"}, {"start_time": 172.02, "end_time": 172.14, "text": "we"}, {"start_time": 172.14, "end_time": 172.26, "text": "get"}, {"start_time": 172.26, "end_time": 172.46, "text": "the"}, {"start_time": 172.46, "end_time": 173.1, "text": "LLM"}, {"start_time": 173.18, "end_time": 173.46, "text": "to"}, {"start_time": 173.46, "end_time": 173.7, "text": "use"}, {"start_time": 173.7, "end_time": 173.94, "text": "the"}, {"start_time": 173.94, "end_time": 174.34, "text": "vector"}, {"start_time": 174.34, "end_time": 174.82, "text": "database"}, {"start_time": 174.82, "end_time": 174.98, "text": "that"}, {"start_time": 174.98, "end_time": 175.34, "text": "contains"}, {"start_time": 175.34, "end_time": 175.54, "text": "the"}, {"start_time": 175.54, "end_time": 175.74, "text": "data"}, {"start_time": 175.74, "end_time": 175.94, "text": "that"}, {"start_time": 175.94, "end_time": 176.1, "text": "will"}, {"start_time": 176.1, "end_time": 176.26, "text": "be"}, {"start_time": 176.26, "end_time": 176.46, "text": "most"}, {"start_time": 176.46, "end_time": 176.9, "text": "relevant"}, {"start_time": 176.9, "end_time": 177.02, "text": "to"}, {"start_time": 177.02, "end_time": 177.14, "text": "the"}, {"start_time": 177.14, "end_time": 177.5, "text": "query?"}, {"start_time": 178.06, "end_time": 178.42, "text": "Let's"}, {"start_time": 178.42, "end_time": 178.58, "text": "add"}, {"start_time": 178.58, "end_time": 178.78, "text": "that"}, {"start_time": 178.78, "end_time": 179.14, "text": "agent"}, {"start_time": 179.14, "end_time": 179.34, "text": "into"}, {"start_time": 179.34, "end_time": 179.54, "text": "this"}, {"start_time": 179.54, "end_time": 180.14, "text": "pipeline."}, {"start_time": 185.42, "end_time": 185.82, "text": "Now,"}, {"start_time": 185.82, "end_time": 186.14, "text": "this"}, {"start_time": 186.14, "end_time": 186.62, "text": "agent"}, {"start_time": 186.62, "end_time": 186.86, "text": "can"}, {"start_time": 186.86, "end_time": 187.5, "text": "intelligently"}, {"start_time": 187.5, "end_time": 187.98, "text": "decide"}, {"start_time": 187.98, "end_time": 188.26, "text": "which"}, {"start_time": 188.26, "end_time": 188.9, "text": "database"}, {"start_time": 188.9, "end_time": 189.02, "text": "to"}, {"start_time": 189.02, "end_time": 189.3, "text": "query"}, {"start_time": 189.3, "end_time": 189.58, "text": "based"}, {"start_time": 189.58, "end_time": 189.74, "text": "on"}, {"start_time": 189.74, "end_time": 189.82, "text": "the"}, {"start_time": 189.82, "end_time": 190.22, "text": "user's"}, {"start_time": 190.22, "end_time": 190.54, "text": "question."}, {"start_time": 190.94, "end_time": 191.22, "text": "And"}, {"start_time": 191.22, "end_time": 191.34, "text": "the"}, {"start_time": 191.34, "end_time": 191.62, "text": "agent"}, {"start_time": 191.62, "end_time": 191.86, "text": "isn't"}, {"start_time": 191.86, "end_time": 192.06, "text": "making"}, {"start_time": 192.06, "end_time": 192.26, "text": "a"}, {"start_time": 192.26, "end_time": 192.62, "text": "random"}, {"start_time": 192.62, "end_time": 192.94, "text": "guess,"}, {"start_time": 192.94, "end_time": 193.34, "text": "it's"}, {"start_time": 193.34, "end_time": 193.98, "text": "leveraging"}, {"start_time": 193.98, "end_time": 194.3, "text": "the"}, {"start_time": 194.3, "end_time": 195.06, "text": "LLM's"}, {"start_time": 195.06, "end_time": 195.54, "text": "language"}, {"start_time": 195.54, "end_time": 196.14, "text": "understanding"}, {"start_time": 196.14, "end_time": 196.62, "text": "capabilities"}, {"start_time": 197.33, "end_time": 197.49, "text": "to"}, {"start_time": 197.49, "end_time": 197.93, "text": "interpret"}, {"start_time": 197.93, "end_time": 198.09, "text": "the"}, {"start_time": 198.09, "end_time": 198.45, "text": "query"}, {"start_time": 198.77, "end_time": 199.09, "text": "and"}, {"start_time": 199.09, "end_time": 199.57, "text": "determine"}, {"start_time": 199.57, "end_time": 199.81, "text": "its"}, {"start_time": 199.81, "end_time": 200.29, "text": "context."}, {"start_time": 201.25, "end_time": 201.49, "text": "So"}, {"start_time": 201.49, "end_time": 201.61, "text": "if"}, {"start_time": 201.61, "end_time": 201.77, "text": "an"}, {"start_time": 201.77, "end_time": 202.25, "text": "employee"}, {"start_time": 202.25, "end_time": 202.73, "text": "asks"}, {"start_time": 202.73, "end_time": 203.09, "text": "what's"}, {"start_time": 203.09, "end_time": 203.21, "text": "the"}, {"start_time": 203.21, "end_time": 203.65, "text": "company's"}, {"start_time": 203.65, "end_time": 203.85, "text": "policy"}, {"start_time": 203.85, "end_time": 204.13, "text": "on"}, {"start_time": 204.13, "end_time": 204.45, "text": "remote"}, {"start_time": 204.45, "end_time": 204.69, "text": "work"}, {"start_time": 204.69, "end_time": 204.93, "text": "during"}, {"start_time": 204.93, "end_time": 205.13, "text": "the"}, {"start_time": 205.13, "end_time": 205.73, "text": "holidays?"}, {"start_time": 205.73, "end_time": 206.01, "text": "It"}, {"start_time": 206.01, "end_time": 206.17, "text": "would"}, {"start_time": 206.17, "end_time": 206.41, "text": "route"}, {"start_time": 206.41, "end_time": 206.61, "text": "that"}, {"start_time": 206.61, "end_time": 206.81, "text": "to"}, {"start_time": 206.81, "end_time": 206.97, "text": "the"}, {"start_time": 206.97, "end_time": 207.37, "text": "internal"}, {"start_time": 207.37, "end_time": 208.05, "text": "documentation"}, {"start_time": 208.13, "end_time": 208.45, "text": "and"}, {"start_time": 208.45, "end_time": 208.69, "text": "that"}, {"start_time": 208.69, "end_time": 209.25, "text": "response"}, {"start_time": 209.25, "end_time": 209.57, "text": "will"}, {"start_time": 209.57, "end_time": 209.77, "text": "be"}, {"start_time": 209.77, "end_time": 209.97, "text": "used"}, {"start_time": 209.97, "end_time": 210.21, "text": "as"}, {"start_time": 210.21, "end_time": 210.61, "text": "context"}, {"start_time": 210.61, "end_time": 210.77, "text": "for"}, {"start_time": 210.77, "end_time": 210.89, "text": "the"}, {"start_time": 210.89, "end_time": 211.25, "text": "prompt."}, {"start_time": 211.57, "end_time": 211.85, "text": "But"}, {"start_time": 211.85, "end_time": 212.01, "text": "if"}, {"start_time": 212.01, "end_time": 212.13, "text": "the"}, {"start_time": 212.13, "end_time": 212.29, "text": "question"}, {"start_time": 212.29, "end_time": 212.49, "text": "is"}, {"start_time": 212.49, "end_time": 212.69, "text": "more"}, {"start_time": 212.69, "end_time": 213.01, "text": "general,"}, {"start_time": 213.089, "end_time": 213.49, "text": "like"}, {"start_time": 213.81, "end_time": 214.09, "text": "what"}, {"start_time": 214.09, "end_time": 214.25, "text": "are"}, {"start_time": 214.25, "end_time": 214.41, "text": "the"}, {"start_time": 214.41, "end_time": 214.69, "text": "industry"}, {"start_time": 214.77, "end_time": 215.41, "text": "standards"}, {"start_time": 215.57, "end_time": 215.93, "text": "for"}, {"start_time": 215.93, "end_time": 216.37, "text": "remote"}, {"start_time": 216.37, "end_time": 216.61, "text": "work"}, {"start_time": 216.61, "end_time": 216.81, "text": "and"}, {"start_time": 216.81, "end_time": 217.09, "text": "tech"}, {"start_time": 217.09, "end_time": 217.41, "text": "companies?"}, {"start_time": 218.37, "end_time": 218.65, "text": "The"}, {"start_time": 218.65, "end_time": 219.09, "text": "agent's"}, {"start_time": 219.09, "end_time": 219.17, "text": "going"}, {"start_time": 219.17, "end_time": 219.25, "text": "to"}, {"start_time": 219.25, "end_time": 219.45, "text": "route"}, {"start_time": 219.45, "end_time": 219.61, "text": "that"}, {"start_time": 219.61, "end_time": 219.73, "text": "to"}, {"start_time": 219.73, "end_time": 219.85, "text": "the"}, {"start_time": 219.85, "end_time": 220.09, "text": "general"}, {"start_time": 220.09, "end_time": 220.49, "text": "knowledge"}, {"start_time": 220.49, "end_time": 221.05, "text": "database"}, {"start_time": 221.05, "end_time": 221.29, "text": "and"}, {"start_time": 221.29, "end_time": 221.53, "text": "that"}, {"start_time": 221.53, "end_time": 221.97, "text": "context"}, {"start_time": 221.97, "end_time": 222.17, "text": "is"}, {"start_time": 222.17, "end_time": 222.29, "text": "going"}, {"start_time": 222.29, "end_time": 222.37, "text": "to"}, {"start_time": 222.37, "end_time": 222.49, "text": "be"}, {"start_time": 222.49, "end_time": 222.77, "text": "used"}, {"start_time": 223.07, "end_time": 223.27, "text": "within"}, {"start_time": 223.27, "end_time": 223.51, "text": "that"}, {"start_time": 223.51, "end_time": 223.87, "text": "prompt."}, {"start_time": 224.43, "end_time": 224.83, "text": "Powered"}, {"start_time": 224.83, "end_time": 224.95, "text": "by"}, {"start_time": 224.95, "end_time": 225.15, "text": "an"}, {"start_time": 225.15, "end_time": 225.67, "text": "LLM"}, {"start_time": 225.67, "end_time": 225.99, "text": "and"}, {"start_time": 225.99, "end_time": 226.51, "text": "properly"}, {"start_time": 226.51, "end_time": 226.91, "text": "trained,"}, {"start_time": 226.91, "end_time": 227.23, "text": "the"}, {"start_time": 227.23, "end_time": 227.67, "text": "agent"}, {"start_time": 227.67, "end_time": 228.31, "text": "analyzes"}, {"start_time": 228.31, "end_time": 228.47, "text": "the"}, {"start_time": 228.47, "end_time": 228.83, "text": "query"}, {"start_time": 228.83, "end_time": 229.19, "text": "and"}, {"start_time": 229.19, "end_time": 229.47, "text": "based"}, {"start_time": 229.47, "end_time": 229.71, "text": "on"}, {"start_time": 229.71, "end_time": 229.91, "text": "the"}, {"start_time": 229.91, "end_time": 230.43, "text": "understanding"}, {"start_time": 230.43, "end_time": 230.55, "text": "of"}, {"start_time": 230.55, "end_time": 230.71, "text": "the"}, {"start_time": 230.71, "end_time": 230.99, "text": "content"}, {"start_time": 231.15, "end_time": 231.47, "text": "and"}, {"start_time": 231.47, "end_time": 231.67, "text": "the"}, {"start_time": 231.67, "end_time": 232.11, "text": "context,"}, {"start_time": 232.11, "end_time": 232.59, "text": "decides"}, {"start_time": 232.59, "end_time": 232.79, "text": "which"}, {"start_time": 232.79, "end_time": 233.31, "text": "database"}, {"start_time": 233.31, "end_time": 233.47, "text": "to"}, {"start_time": 233.47, "end_time": 233.71, "text": "use."}, {"start_time": 234.27, "end_time": 234.55, "text": "But"}, {"start_time": 234.55, "end_time": 234.75, "text": "they're"}, {"start_time": 234.75, "end_time": 234.87, "text": "not"}, {"start_time": 234.87, "end_time": 235.07, "text": "always"}, {"start_time": 235.07, "end_time": 235.23, "text": "going"}, {"start_time": 235.23, "end_time": 235.31, "text": "to"}, {"start_time": 235.31, "end_time": 235.47, "text": "ask"}, {"start_time": 235.47, "end_time": 235.91, "text": "questions"}, {"start_time": 235.91, "end_time": 236.11, "text": "that"}, {"start_time": 236.11, "end_time": 236.43, "text": "are"}, {"start_time": 236.83, "end_time": 237.47, "text": "generally"}, {"start_time": 237.47, "end_time": 237.75, "text": "or"}, {"start_time": 237.75, "end_time": 238.39, "text": "genuinely"}, {"start_time": 238.39, "end_time": 238.79, "text": "relevant"}, {"start_time": 238.79, "end_time": 239.07, "text": "to"}, {"start_time": 239.55, "end_time": 239.83, "text": "any"}, {"start_time": 239.83, "end_time": 239.95, "text": "of"}, {"start_time": 239.95, "end_time": 240.03, "text": "the"}, {"start_time": 240.03, "end_time": 240.19, "text": "stuff"}, {"start_time": 240.19, "end_time": 240.31, "text": "that"}, {"start_time": 240.31, "end_time": 240.43, "text": "we"}, {"start_time": 240.43, "end_time": 240.55, "text": "have"}, {"start_time": 240.55, "end_time": 240.67, "text": "in"}, {"start_time": 240.67, "end_time": 240.79, "text": "our"}, {"start_time": 240.79, "end_time": 241.07, "text": "vector"}, {"start_time": 241.07, "end_time": 241.39, "text": "db."}, {"start_time": 241.39, "end_time": 241.67, "text": "So"}, {"start_time": 241.67, "end_time": 241.83, "text": "what"}, {"start_time": 241.83, "end_time": 242.11, "text": "if"}, {"start_time": 242.19, "end_time": 242.55, "text": "someone"}, {"start_time": 242.55, "end_time": 242.95, "text": "asks"}, {"start_time": 242.95, "end_time": 243.11, "text": "a"}, {"start_time": 243.11, "end_time": 243.39, "text": "question"}, {"start_time": 243.63, "end_time": 243.91, "text": "that"}, {"start_time": 243.91, "end_time": 244.07, "text": "is"}, {"start_time": 244.07, "end_time": 244.23, "text": "just"}, {"start_time": 244.23, "end_time": 244.55, "text": "totally"}, {"start_time": 244.55, "end_time": 244.71, "text": "out"}, {"start_time": 244.71, "end_time": 244.83, "text": "of"}, {"start_time": 244.83, "end_time": 244.99, "text": "left"}, {"start_time": 244.99, "end_time": 245.31, "text": "field,"}, {"start_time": 245.63, "end_time": 245.95, "text": "like"}, {"start_time": 245.95, "end_time": 246.15, "text": "who"}, {"start_time": 246.15, "end_time": 246.31, "text": "won"}, {"start_time": 246.31, "end_time": 246.47, "text": "the"}, {"start_time": 246.47, "end_time": 246.67, "text": "world"}, {"start_time": 246.67, "end_time": 246.99, "text": "series"}, {"start_time": 246.99, "end_time": 247.27, "text": "in"}, {"start_time": 247.27, "end_time": 248.03, "text": "2015?"}, {"start_time": 249.0, "end_time": 249.16, "text": "What"}, {"start_time": 249.16, "end_time": 249.32, "text": "the"}, {"start_time": 249.32, "end_time": 249.6, "text": "agent"}, {"start_time": 249.6, "end_time": 249.76, "text": "can"}, {"start_time": 249.76, "end_time": 249.92, "text": "do"}, {"start_time": 249.92, "end_time": 250.08, "text": "at"}, {"start_time": 250.08, "end_time": 250.28, "text": "that"}, {"start_time": 250.28, "end_time": 250.6, "text": "point"}, {"start_time": 250.84, "end_time": 251.12, "text": "is"}, {"start_time": 251.12, "end_time": 251.28, "text": "it"}, {"start_time": 251.28, "end_time": 251.48, "text": "could"}, {"start_time": 251.48, "end_time": 251.76, "text": "route"}, {"start_time": 251.76, "end_time": 252.04, "text": "it"}, {"start_time": 252.36, "end_time": 252.64, "text": "to"}, {"start_time": 252.64, "end_time": 252.8, "text": "a"}, {"start_time": 252.8, "end_time": 253.0, "text": "fail"}, {"start_time": 253.0, "end_time": 253.56, "text": "safe."}, {"start_time": 255.64, "end_time": 255.88, "text": "So"}, {"start_time": 255.88, "end_time": 256.04, "text": "because"}, {"start_time": 256.04, "end_time": 256.24, "text": "the"}, {"start_time": 256.24, "end_time": 256.52, "text": "agent"}, {"start_time": 256.52, "end_time": 256.72, "text": "is"}, {"start_time": 256.72, "end_time": 256.88, "text": "able"}, {"start_time": 256.88, "end_time": 257.04, "text": "to"}, {"start_time": 257.04, "end_time": 257.64, "text": "recognize"}, {"start_time": 258.68, "end_time": 259.08, "text": "the"}, {"start_time": 259.08, "end_time": 259.72, "text": "context"}, {"start_time": 259.72, "end_time": 260.12, "text": "of"}, {"start_time": 260.36, "end_time": 260.64, "text": "the"}, {"start_time": 260.64, "end_time": 261.0, "text": "query,"}, {"start_time": 261.64, "end_time": 261.92, "text": "it"}, {"start_time": 261.92, "end_time": 262.12, "text": "could"}, {"start_time": 262.12, "end_time": 262.56, "text": "recognize"}, {"start_time": 262.56, "end_time": 262.68, "text": "that"}, {"start_time": 262.68, "end_time": 262.84, "text": "it's"}, {"start_time": 262.84, "end_time": 263.0, "text": "not"}, {"start_time": 263.0, "end_time": 263.16, "text": "a"}, {"start_time": 263.16, "end_time": 263.32, "text": "part"}, {"start_time": 263.32, "end_time": 263.48, "text": "of"}, {"start_time": 263.48, "end_time": 263.6, "text": "the"}, {"start_time": 263.6, "end_time": 263.76, "text": "two"}, {"start_time": 263.76, "end_time": 264.28, "text": "databases"}, {"start_time": 264.28, "end_time": 264.519, "text": "that"}, {"start_time": 264.519, "end_time": 264.719, "text": "we"}, {"start_time": 264.719, "end_time": 265.0, "text": "have"}, {"start_time": 265.16, "end_time": 265.48, "text": "get"}, {"start_time": 265.48, "end_time": 265.88, "text": "routed"}, {"start_time": 265.88, "end_time": 266.0, "text": "to"}, {"start_time": 266.0, "end_time": 266.12, "text": "the"}, {"start_time": 266.12, "end_time": 266.28, "text": "fail"}, {"start_time": 266.28, "end_time": 266.84, "text": "safe"}, {"start_time": 267.16, "end_time": 267.48, "text": "and"}, {"start_time": 267.48, "end_time": 267.8, "text": "return"}, {"start_time": 267.88, "end_time": 268.28, "text": "back,"}, {"start_time": 269.4, "end_time": 269.96, "text": "Sorry,"}, {"start_time": 270.6, "end_time": 270.88, "text": "I"}, {"start_time": 270.88, "end_time": 271.08, "text": "don't"}, {"start_time": 271.08, "end_time": 271.16, "text": "have"}, {"start_time": 271.16, "end_time": 271.28, "text": "the"}, {"start_time": 271.28, "end_time": 271.56, "text": "information"}, {"start_time": 271.64, "end_time": 271.96, "text": "you're"}, {"start_time": 271.96, "end_time": 272.12, "text": "looking"}, {"start_time": 272.12, "end_time": 272.44, "text": "for."}, {"start_time": 272.68, "end_time": 273.0, "text": "This"}, {"start_time": 273.0, "end_time": 273.52, "text": "agentic"}, {"start_time": 273.52, "end_time": 273.88, "text": "RAG"}, {"start_time": 273.88, "end_time": 274.28, "text": "pipeline"}, {"start_time": 274.28, "end_time": 274.4, "text": "can"}, {"start_time": 274.4, "end_time": 274.52, "text": "be"}, {"start_time": 274.52, "end_time": 274.68, "text": "used"}, {"start_time": 274.68, "end_time": 274.96, "text": "in"}, {"start_time": 274.96, "end_time": 275.44, "text": "customer"}, {"start_time": 275.44, "end_time": 275.72, "text": "support"}, {"start_time": 275.72, "end_time": 276.2, "text": "systems"}, {"start_time": 276.2, "end_time": 276.44, "text": "and"}, {"start_time": 276.44, "end_time": 276.76, "text": "legal"}, {"start_time": 276.76, "end_time": 277.08, "text": "tech."}, {"start_time": 277.81, "end_time": 277.93, "text": "For"}, {"start_time": 277.93, "end_time": 278.29, "text": "example,"}, {"start_time": 278.29, "end_time": 278.49, "text": "a"}, {"start_time": 278.49, "end_time": 278.81, "text": "lawyer"}, {"start_time": 278.81, "end_time": 278.97, "text": "can"}, {"start_time": 278.97, "end_time": 279.25, "text": "source"}, {"start_time": 279.25, "end_time": 279.73, "text": "answers"}, {"start_time": 279.73, "end_time": 280.01, "text": "to"}, {"start_time": 280.01, "end_time": 280.21, "text": "their"}, {"start_time": 280.21, "end_time": 280.57, "text": "questions"}, {"start_time": 280.57, "end_time": 280.77, "text": "from"}, {"start_time": 280.77, "end_time": 280.97, "text": "like"}, {"start_time": 280.97, "end_time": 281.17, "text": "their"}, {"start_time": 281.17, "end_time": 281.65, "text": "internal"}, {"start_time": 281.65, "end_time": 282.21, "text": "briefs"}, {"start_time": 282.45, "end_time": 282.73, "text": "and"}, {"start_time": 282.73, "end_time": 282.93, "text": "then"}, {"start_time": 282.93, "end_time": 283.17, "text": "in"}, {"start_time": 283.17, "end_time": 283.45, "text": "another"}, {"start_time": 283.45, "end_time": 283.81, "text": "query"}, {"start_time": 283.81, "end_time": 284.01, "text": "just"}, {"start_time": 284.01, "end_time": 284.21, "text": "get"}, {"start_time": 284.21, "end_time": 284.49, "text": "stuff"}, {"start_time": 284.49, "end_time": 284.73, "text": "from"}, {"start_time": 284.73, "end_time": 285.05, "text": "public"}, {"start_time": 285.05, "end_time": 285.61, "text": "caseload"}, {"start_time": 285.61, "end_time": 286.21, "text": "databases."}, {"start_time": 286.53, "end_time": 286.81, "text": "The"}, {"start_time": 286.81, "end_time": 287.13, "text": "agent"}, {"start_time": 287.13, "end_time": 287.29, "text": "can"}, {"start_time": 287.29, "end_time": 287.41, "text": "be"}, {"start_time": 287.41, "end_time": 287.81, "text": "utilized"}, {"start_time": 287.81, "end_time": 288.01, "text": "in"}, {"start_time": 288.01, "end_time": 288.13, "text": "a"}, {"start_time": 288.13, "end_time": 288.29, "text": "ton"}, {"start_time": 288.29, "end_time": 288.41, "text": "of"}, {"start_time": 288.41, "end_time": 288.85, "text": "ways."}, {"start_time": 289.17, "end_time": 289.93, "text": "Agentic"}, {"start_time": 289.93, "end_time": 290.37, "text": "rag"}, {"start_time": 290.37, "end_time": 290.65, "text": "is"}, {"start_time": 290.65, "end_time": 290.81, "text": "an"}, {"start_time": 290.81, "end_time": 291.25, "text": "evolution"}, {"start_time": 291.25, "end_time": 291.45, "text": "in"}, {"start_time": 291.45, "end_time": 291.61, "text": "how"}, {"start_time": 291.61, "end_time": 291.77, "text": "we"}, {"start_time": 291.77, "end_time": 292.09, "text": "enhance"}, {"start_time": 292.09, "end_time": 292.25, "text": "the"}, {"start_time": 292.25, "end_time": 292.61, "text": "rag"}, {"start_time": 292.61, "end_time": 293.09, "text": "pipeline"}, {"start_time": 293.09, "end_time": 293.289, "text": "by"}, {"start_time": 293.289, "end_time": 293.57, "text": "moving"}, {"start_time": 293.57, "end_time": 294.05, "text": "beyond"}, {"start_time": 294.21, "end_time": 294.73, "text": "simple"}, {"start_time": 294.73, "end_time": 295.29, "text": "response"}, {"start_time": 295.29, "end_time": 295.81, "text": "generation"}, {"start_time": 295.81, "end_time": 296.13, "text": "to"}, {"start_time": 296.13, "end_time": 296.37, "text": "more"}, {"start_time": 296.37, "end_time": 296.97, "text": "intelligent"}, {"start_time": 296.97, "end_time": 297.33, "text": "decision"}, {"start_time": 297.41, "end_time": 297.81, "text": "making."}, {"start_time": 298.37, "end_time": 298.69, "text": "By"}, {"start_time": 298.69, "end_time": 299.09, "text": "allowing"}, {"start_time": 299.09, "end_time": 299.29, "text": "an"}, {"start_time": 299.29, "end_time": 299.61, "text": "agent"}, {"start_time": 299.61, "end_time": 299.77, "text": "to"}, {"start_time": 299.77, "end_time": 300.05, "text": "choose"}, {"start_time": 300.05, "end_time": 300.25, "text": "the"}, {"start_time": 300.25, "end_time": 300.45, "text": "best"}, {"start_time": 300.45, "end_time": 300.73, "text": "data"}, {"start_time": 300.73, "end_time": 301.17, "text": "sources"}, {"start_time": 301.41, "end_time": 301.73, "text": "and"}, {"start_time": 301.73, "end_time": 302.17, "text": "potentially"}, {"start_time": 302.17, "end_time": 302.37, "text": "even"}, {"start_time": 302.37, "end_time": 302.97, "text": "incorporate"}, {"start_time": 302.97, "end_time": 303.49, "text": "external"}, {"start_time": 303.49, "end_time": 303.81, "text": "information"}, {"start_time": 304.61, "end_time": 304.85, "text": "like"}, {"start_time": 304.85, "end_time": 305.17, "text": "real"}, {"start_time": 305.17, "end_time": 305.41, "text": "time"}, {"start_time": 305.41, "end_time": 305.73, "text": "data"}, {"start_time": 305.73, "end_time": 306.01, "text": "or"}, {"start_time": 306.01, "end_time": 306.25, "text": "third"}, {"start_time": 306.25, "end_time": 306.57, "text": "party"}, {"start_time": 306.57, "end_time": 306.93, "text": "services,"}, {"start_time": 307.57, "end_time": 307.85, "text": "we"}, {"start_time": 307.85, "end_time": 308.05, "text": "can"}, {"start_time": 308.05, "end_time": 308.37, "text": "create"}, {"start_time": 308.69, "end_time": 308.97, "text": "a"}, {"start_time": 308.97, "end_time": 309.45, "text": "pipeline"}, {"start_time": 309.45, "end_time": 309.69, "text": "that's"}, {"start_time": 309.69, "end_time": 309.89, "text": "more"}, {"start_time": 309.89, "end_time": 310.53, "text": "responsive,"}, {"start_time": 310.77, "end_time": 311.09, "text": "more"}, {"start_time": 311.09, "end_time": 311.65, "text": "accurate"}, {"start_time": 311.65, "end_time": 311.93, "text": "and"}, {"start_time": 311.93, "end_time": 312.21, "text": "more"}, {"start_time": 312.21, "end_time": 312.85, "text": "adaptable."}, {"start_time": 313.25, "end_time": 313.57, "text": "This"}, {"start_time": 313.57, "end_time": 313.89, "text": "approach"}, {"start_time": 313.89, "end_time": 314.37, "text": "opens"}, {"start_time": 314.37, "end_time": 314.57, "text": "up"}, {"start_time": 314.57, "end_time": 314.81, "text": "so"}, {"start_time": 314.81, "end_time": 315.01, "text": "many"}, {"start_time": 315.01, "end_time": 315.57, "text": "possibilities"}, {"start_time": 315.57, "end_time": 315.89, "text": "for"}, {"start_time": 315.89, "end_time": 316.37, "text": "applications"}, {"start_time": 316.37, "end_time": 316.61, "text": "in"}, {"start_time": 316.61, "end_time": 317.05, "text": "customer"}, {"start_time": 317.05, "end_time": 317.33, "text": "service,"}, {"start_time": 317.49, "end_time": 318.01, "text": "legal,"}, {"start_time": 318.01, "end_time": 318.33, "text": "tech,"}, {"start_time": 318.33, "end_time": 319.01, "text": "healthcare,"}, {"start_time": 319.49, "end_time": 319.97, "text": "virtually"}, {"start_time": 319.97, "end_time": 320.249, "text": "any"}, {"start_time": 320.249, "end_time": 320.61, "text": "field."}, {"start_time": 321.17, "end_time": 321.45, "text": "As"}, {"start_time": 321.45, "end_time": 321.65, "text": "the"}, {"start_time": 321.65, "end_time": 321.97, "text": "technology"}, {"start_time": 322.13, "end_time": 322.65, "text": "continues"}, {"start_time": 322.65, "end_time": 322.81, "text": "to"}, {"start_time": 322.81, "end_time": 323.25, "text": "evolve,"}, {"start_time": 323.25, "end_time": 323.53, "text": "we"}, {"start_time": 323.53, "end_time": 323.73, "text": "will"}, {"start_time": 323.73, "end_time": 323.97, "text": "see"}, {"start_time": 323.97, "end_time": 324.37, "text": "AI"}, {"start_time": 324.37, "end_time": 324.81, "text": "systems"}, {"start_time": 324.81, "end_time": 325.01, "text": "that"}, {"start_time": 325.01, "end_time": 325.49, "text": "truly"}, {"start_time": 325.49, "end_time": 325.81, "text": "understand"}, {"start_time": 325.97, "end_time": 326.61, "text": "context"}, {"start_time": 326.61, "end_time": 326.89, "text": "and"}, {"start_time": 326.89, "end_time": 327.09, "text": "can"}, {"start_time": 327.09, "end_time": 327.41, "text": "deliver"}, {"start_time": 327.41, "end_time": 328.01, "text": "amazing"}, {"start_time": 328.01, "end_time": 328.53, "text": "values"}, {"start_time": 328.53, "end_time": 328.65, "text": "to"}, {"start_time": 328.65, "end_time": 328.77, "text": "the"}, {"start_time": 328.77, "end_time": 328.93, "text": "end"}, {"start_time": 328.93, "end_time": 329.41, "text": "user."}, {"start_time": 330.06, "end_time": 330.3, "text": "SA."}], "conversation_history": [{"role": "user", "content": "hey", "timestamp": 1751720791.3180249}, {"role": "assistant", "content": "Hi there, how can I help you with your video content today?", "timestamp": 1751720791.318026}, {"role": "user", "content": "what is that video about?", "timestamp": 1751720812.294508}, {"role": "assistant", "content": "The video is about an Agentic RAG (Re-ranking with Aggregation and Generation) pipeline and its application in improving the quality and reliability of responses from a Large Language Model (LLM).", "timestamp": 1751720812.2945092}, {"role": "user", "content": "can you provide me more detail to understand the video content", "timestamp": 1751720855.89782}, {"role": "assistant", "content": "The video discusses an Agentic RAG pipeline that improves the quality and reliability of responses from a Large Language Model (LLM). \n\nHere's a breakdown of the main points:\n\n1. The pipeline grounds LLM responses in concrete and accurate information, enhancing response quality and reliability.\n2. The agent takes an active role, making decisions to improve data relevance and accuracy.\n3. The agent analyzes queries, decides which database to use, and retrieves data based on content and context understanding.\n4. The technology has the potential to evolve into AI systems that truly understand context and deliver value to users.\n5. The pipeline can be augmented with multiple sources of data, such as internal documentation and external databases.\n6. This approach has various applications across industries, including customer service, legal, tech, and healthcare.", "timestamp": 1751720855.897822}, {"role": "user", "content": "What is this video about", "timestamp": 1751726253.406485}, {"role": "assistant", "content": "The video is about an Agentic RAG (Re-ranking with Aggregation and Generation) pipeline and its application in improving the quality and reliability of responses from a Large Language Model (LLM).", "timestamp": 1751726253.4064891}, {"role": "user", "content": "hey", "timestamp": 1751726321.765693}, {"role": "assistant", "content": "Hi there. I'm happy to help with any questions you have about video content. How can I assist you today?", "timestamp": 1751726321.765695}, {"role": "user", "content": "what is RAG", "timestamp": 1751726329.896405}, {"role": "assistant", "content": "Retrieval augmented generation (RAG) is a powerful and popular pipeline that enhances responses from a large language model. It incorporates relevant data retrieved from a vector database, adds it as context to the prompt, and sends it to the LLM for generation. This allows the LLM to ground its response in concrete and accurate information, improving the quality and reliability of the response.", "timestamp": 1751726329.896406}, {"role": "user", "content": "What is this video about", "timestamp": 1751726339.325826}, {"role": "assistant", "content": "The video is about an Agentic RAG (Re-ranking with Aggregation and Generation) pipeline and its application in improving the quality and reliability of responses from a Large Language Model (LLM).", "timestamp": 1751726339.3258271}], "error": null, "topics": [{"title": "Enhancing LLM Responses with Retrieval Augmentation", "start_time": 1.12, "end_time": 107.92, "text": "So we all know what retrieval augmented generation is, but let's just do a quick refresher. Retrieval augmented generation is a powerful and popular pipeline that enhances responses from a large language model. It does this by incorporating relevant data retrieved from a vector database, adding it as context to the prompt, and sending it to the LLM for generation. What this does is it allows the LLM to ground its response in concrete and accurate information, and that improves the quality and reliability of the response. Let me quickly sketch it out. So let's say we have a user or an application even, and they send a query. Now, without retrieval augmented generation, this query is going to go and get itself interpolated into a prompt, and from there that's going to hit the LLM and that's going to generate an output to make this rag, we can add a vector database. So instead of just going directly and getting itself interpolated into the prompt, it's going to hit this vector db. And the response from that vector DB is going to be used as context for the prompt. Now, in this typical RAG pipeline, we call the LLM only once, and we use it solely to generate a response. But what if we could leverage the LLM not just for responses, but also for additional tasks, like deciding which vector database to query if we have multiple databases, or even determining the type of response to give should it answer with text, generate a chart, or even provide a code snippet?", "segments": [{"text": "So we all know what retrieval augmented generation is, but let's just do a quick refresher.", "start_time": 1.12, "end_time": 6.4}, {"text": "Retrieval augmented generation is a powerful and popular pipeline that enhances responses from a large language model.", "start_time": 6.88, "end_time": 14.0}, {"text": "It does this by incorporating relevant data retrieved from a vector database, adding it as context to the prompt, and sending it to the LLM for generation.", "start_time": 14.16, "end_time": 22.24}, {"text": "What this does is it allows the LLM to ground its response in concrete and accurate information, and that improves the quality and reliability of the response.", "start_time": 22.72, "end_time": 31.53}, {"text": "Let me quickly sketch it out.", "start_time": 31.69, "end_time": 33.05}, {"text": "So let's say we have a user or an application even, and they send a query.", "start_time": 34.41, "end_time": 42.33}, {"text": "Now, without retrieval augmented generation, this query is going to go and get itself interpolated into a prompt, and from there that's going to hit the LLM and that's going to generate an output to make this rag, we can add a vector database.", "start_time": 44.57, "end_time": 71.46}, {"text": "So instead of just going directly and getting itself interpolated into the prompt, it's going to hit this vector db.", "start_time": 72.58, "end_time": 77.38}, {"text": "And the response from that vector DB is going to be used as context for the prompt.", "start_time": 77.54, "end_time": 81.94}, {"text": "Now, in this typical RAG pipeline, we call the LLM only once, and we use it solely to generate a response.", "start_time": 83.36, "end_time": 90.08}, {"text": "But what if we could leverage the LLM not just for responses, but also for additional tasks, like deciding which vector database to query if we have multiple databases, or even determining the type of response to give should it answer with text, generate a chart, or even provide a code snippet?", "start_time": 90.8, "end_time": 107.92}]}, {"title": "Augmenting Data Retrieval with Agentic RAG Pipeline", "start_time": 108.07, "end_time": 170.38, "text": "And that would all be dependent on the context of that query. So this is where the agentic RAG pipeline comes into play. In agentic rag, we use the LLM as an agent, and the LLM goes beyond just generating a response. It takes on an active role and can make decisions that will improve both the relevance and accuracy of the retrieved data. Now let's explore how we can augment the initial process with an agent and a couple of different sources of data. So instead of just one single source, let's add a second. And the first one can be internal documentation, right? And the second one can be general industry knowledge. Now, in the internal documentation, we're going to have things like policies, procedures and guidelines. And the general knowledge base will have things like industry standards, best practices and public resources.", "segments": [{"text": "And that would all be dependent on the context of that query.", "start_time": 108.07, "end_time": 111.11}, {"text": "So this is where the agentic RAG pipeline comes into play.", "start_time": 112.31, "end_time": 118.87}, {"text": "In agentic rag, we use the LLM as an agent, and the LLM goes beyond just generating a response.", "start_time": 118.87, "end_time": 125.35}, {"text": "It takes on an active role and can make decisions that will improve both the relevance and accuracy of the retrieved data.", "start_time": 125.51, "end_time": 132.87}, {"text": "Now let's explore how we can augment the initial process with an agent and a couple of different sources of data.", "start_time": 133.35, "end_time": 139.44}, {"text": "So instead of just one single source, let's add a second.", "start_time": 140.32, "end_time": 143.44}, {"text": "And the first one can be internal documentation, right?", "start_time": 145.68, "end_time": 151.12}, {"text": "And the second one can be general industry knowledge.", "start_time": 151.6, "end_time": 154.96}, {"text": "Now, in the internal documentation, we're going to have things like policies, procedures and guidelines.", "start_time": 159.84, "end_time": 164.44}, {"text": "And the general knowledge base will have things like industry standards, best practices and public resources.", "start_time": 164.44, "end_time": 170.38}]}, {"title": "Intelligent Query Routing for Vector Database", "start_time": 171.5, "end_time": 241.39, "text": "So how can we get the LLM to use the vector database that contains the data that will be most relevant to the query? Let's add that agent into this pipeline. Now, this agent can intelligently decide which database to query based on the user's question. And the agent isn't making a random guess, it's leveraging the LLM's language understanding capabilities to interpret the query and determine its context. So if an employee asks what's the company's policy on remote work during the holidays? It would route that to the internal documentation and that response will be used as context for the prompt. But if the question is more general, like what are the industry standards for remote work and tech companies? The agent's going to route that to the general knowledge database and that context is going to be used within that prompt. Powered by an LLM and properly trained, the agent analyzes the query and based on the understanding of the content and the context, decides which database to use. But they're not always going to ask questions that are generally or genuinely relevant to any of the stuff that we have in our vector db.", "segments": [{"text": "So how can we get the LLM to use the vector database that contains the data that will be most relevant to the query?", "start_time": 171.5, "end_time": 177.5}, {"text": "Let's add that agent into this pipeline.", "start_time": 178.06, "end_time": 180.14}, {"text": "Now, this agent can intelligently decide which database to query based on the user's question.", "start_time": 185.42, "end_time": 190.54}, {"text": "And the agent isn't making a random guess, it's leveraging the LLM's language understanding capabilities to interpret the query and determine its context.", "start_time": 190.94, "end_time": 200.29}, {"text": "So if an employee asks what's the company's policy on remote work during the holidays?", "start_time": 201.25, "end_time": 205.73}, {"text": "It would route that to the internal documentation and that response will be used as context for the prompt.", "start_time": 205.73, "end_time": 211.25}, {"text": "But if the question is more general, like what are the industry standards for remote work and tech companies?", "start_time": 211.57, "end_time": 217.41}, {"text": "The agent's going to route that to the general knowledge database and that context is going to be used within that prompt.", "start_time": 218.37, "end_time": 223.87}, {"text": "Powered by an LLM and properly trained, the agent analyzes the query and based on the understanding of the content and the context, decides which database to use.", "start_time": 224.43, "end_time": 233.71}, {"text": "But they're not always going to ask questions that are generally or genuinely relevant to any of the stuff that we have in our vector db.", "start_time": 234.27, "end_time": 241.39}]}, {"title": "Advanced AI Decision Making for Customer Support", "start_time": 241.39, "end_time": 329.41, "text": "So what if someone asks a question that is just totally out of left field, like who won the world series in 2015? What the agent can do at that point is it could route it to a fail safe. So because the agent is able to recognize the context of the query, it could recognize that it's not a part of the two databases that we have get routed to the fail safe and return back, Sorry, I don't have the information you're looking for. This agentic RAG pipeline can be used in customer support systems and legal tech. For example, a lawyer can source answers to their questions from like their internal briefs and then in another query just get stuff from public caseload databases. The agent can be utilized in a ton of ways. Agentic rag is an evolution in how we enhance the rag pipeline by moving beyond simple response generation to more intelligent decision making. By allowing an agent to choose the best data sources and potentially even incorporate external information like real time data or third party services, we can create a pipeline that's more responsive, more accurate and more adaptable. This approach opens up so many possibilities for applications in customer service, legal, tech, healthcare, virtually any field. As the technology continues to evolve, we will see AI systems that truly understand context and can deliver amazing values to the end user.", "segments": [{"text": "So what if someone asks a question that is just totally out of left field, like who won the world series in 2015?", "start_time": 241.39, "end_time": 248.03}, {"text": "What the agent can do at that point is it could route it to a fail safe.", "start_time": 249.0, "end_time": 253.56}, {"text": "So because the agent is able to recognize the context of the query, it could recognize that it's not a part of the two databases that we have get routed to the fail safe and return back, Sorry, I don't have the information you're looking for.", "start_time": 255.64, "end_time": 272.44}, {"text": "This agentic RAG pipeline can be used in customer support systems and legal tech.", "start_time": 272.68, "end_time": 277.08}, {"text": "For example, a lawyer can source answers to their questions from like their internal briefs and then in another query just get stuff from public caseload databases.", "start_time": 277.81, "end_time": 286.21}, {"text": "The agent can be utilized in a ton of ways.", "start_time": 286.53, "end_time": 288.85}, {"text": "Agentic rag is an evolution in how we enhance the rag pipeline by moving beyond simple response generation to more intelligent decision making.", "start_time": 289.17, "end_time": 297.81}, {"text": "By allowing an agent to choose the best data sources and potentially even incorporate external information like real time data or third party services, we can create a pipeline that's more responsive, more accurate and more adaptable.", "start_time": 298.37, "end_time": 312.85}, {"text": "This approach opens up so many possibilities for applications in customer service, legal, tech, healthcare, virtually any field.", "start_time": 313.25, "end_time": 320.61}, {"text": "As the technology continues to evolve, we will see AI systems that truly understand context and can deliver amazing values to the end user.", "start_time": 321.17, "end_time": 329.41}]}, {"title": "It seems like you haven't provided the transcript section yet. Please share it with", "start_time": 330.06, "end_time": 330.3, "text": "SA.", "segments": [{"text": "SA.", "start_time": 330.06, "end_time": 330.3}]}], "completed_at": 1751720774.98067}