If you're working on language models like GPT4 or Llama 3 and want to build powerful AI applications, you have probably come across LangChain, Langdraph, Langflow and Langsmith. But how do they differ and which one should you use? I'll explain everything you need to know with examples. In this video we'll explore what each technology is, how they are used, compare them with each other, and also answer the most common questions developers have. By the end of this video, you will be clear on which one fits your needs best. If you are interested in learning and building cool tech and AI related stuff, consider subscribing this channel and hit the bell icon so that you don't miss any updates. Let's get started. Let's talk about LangChain first. Let's understand LangChain with the help of an example. Let's say you want to build an application that will use GPT4 to generate an initial response, llama3 to refine that response, an agent that decides whether to fetch external data or generate a response based on the query and memory to store previous interactions with the user. Now without LangChain, you would need to manually manage all of these components and write a lot of code to handle the logic, API calls and memory. And this is how the code would look like. You'll need to write function to make API call to GPT4, another function to make API call to llama3 and then you'll need to create your own memory and manage it, update it, and you'll need to write your own code to create agents and all the related tools that agent would use. You can easily see the challenges with this approach. There will be a lot of boilerplate code and you'll need to manage all the API calls and you need to manually code the agent's decision making process. And as the logic grows complex, the code becomes harder to maintain and scale. And that's where LangChain comes into the picture. LangChain is an open source framework for building applications powered by language models, helping developers chain prompts, interact with external data and build applications that remember context. Let's break down the key features that make LangChain so powerful. LangChain makes it easier to work with LLMs by providing abstractions which are pre built steps and concepts that you can chain together. LLM support it supports almost any LLM, whether it's closed source models like GPT4 or open source ones like Llama 3. You just need to plug in your API key prompts. LangChain has prompt templates so you don't need to hard code any query. You can customize prompts dynamically for different tasks. Chains are the heart of LangChain. You can connect different tasks like calling an LLM, retrieving data, processing responses in one seamless workflow. Indexes LangChain lets you use indexes like document loaders and vector databases to pull in external data so your model aren't limited to what they are trained on. One big advantage is memory. LangChain enables your application to remember past interactions, adding long term memory to your workflows and finally agents. These components use the LLM as a reasoning engine to decide the next step in a workflow, making your application more dynamic and responsive. So in our example this is how the code would look like with LangChain. First we'll need to import LangChain and then we can just use memory class from the LangChain to create and update memory. Then use agent class to create agents and then use sequential chain to create chains and connect various functions. Next, let's talk about Langgraph. Langgraph is built on top of LangChain to manage agents and their workflows. This library enables developers to create agents and multi agent workflows. It is an open source library and so it's free to use. So how is it different from LangChain? While LangChain is great for prompt chaining, Langgraph excels at handling multiple agents in a more structured workflows. If you're building a system where multiple agents interact to solve complex problems, example task automation or research assistants, you can consider using langgraph. Langgraph has a concept of graph which has three core components. First state. The state is a shared data structure that represents the current snapshot of the application. It maintains information that can be updated and accessed by different parts of the graph. A typical state might include user inputs, agent outcomes and a list of actions taken throughout the workflow. Second component is nodes. Nodes represent the individual components or actions within the graph. Each node can perform specific tasks such as executing an LLM, running a function or interacting with external tools. The third core component is edges. Edges connect nodes and and define the flow of execution within the graph. They determine how data moves from one node to another. And this is not a directed graph which means nodes can make decision about which node they want to call next and they can talk to each other back and forth. You can use Landgraph when you need to create agents that require cyclical interactions and decision making processes. It is also ideal for scenarios where multiple agents need to collaborate and work Together. Using Langgraph in your application is really straightforward. You just need to download the package, then import it in your project and then start using the classes provided by this library. Next, let's talk about langflow. Imagine building AI powered apps like Chatbots or data processing tools without having to write code. Well, Langflow makes that possible with its drag and drop interface. Langflow is built on top of LangChain and provides a visual interface to build and experiment with LangChain flows. It's perfect for prototyping LLM applications and it allows users to quickly design workflows, chains, agents and test them. It is mostly not intended to be used in production, but rather for prototyping. It's perfect for teams looking to create minimum viable products quickly. You can consider other tools like relevance, AI or DeFi as well. There are a couple of ways you can use langflow. First one is using Datastax langflow. Of course this is not going to be free, but this is one option. Another option is to install it locally or host it on your cloud server. You can find all these instructions in the Langflow documentation and how to do that. Once it's hosted on your cloud server or datastax, you can access langflow on a UI like this, where you can drag and draw various tools and services. You can connect them together and create an entire AI workflow. You can then access this workflow using APIs from anywhere else. So if you have a separate application from where you want to trigger this workflow, you can do that. You can find all the information how to use this API or the curl on the Langflow documentation. You can find the link to that documentation in the description. And finally, let's talk about Langsmith. Building your LLM based application is one part, but deploying and testing and making sure that all of your agents and LLM calls are working as expected, that they're not overdoing something and they're returning the results as expected, while also monitoring the number of tokens that are used in each request is really important. Without that, publishing your application to the general public is kind of risky. And that's where Langsmith comes into the picture. Langsmith is designed to assist you at all stages of the LLM application's lifecycle. This includes prototyping, beta testing and production. While LangChain focuses on building applications, Langsmith ensures those applications perform well by offering robust monitoring and evaluation tools. Langfuse and Phoenix are other free open source alternatives if you want to explore. Langsmith is designed to be independent so you can use it with any LLM framework and using LangChain is not necessary, which means you can connect Langsmith with LangChain and Langgraph. It provides deeper insights into how the workflows are performing, helping developers find and fix issues. Now, when should you not use Langsmith? If your application is straightforward and doesn't require extensive monitoring or testing, the overhead of Langsmith may not be necessary. Using Langsmith in your project is really straightforward. All you need to do is install the Langsmith, import it in your project, then set up these two environment variables, and then you can start logging your traces from your application. By using traceable annotation, Langsmith will receive all of these traces and in the Langsmith dashboard you'll be able to see all the details on how many tokens were used, how many calls were made, the total cost, error rate and latency details. You can also monitor trends of the number of calls, the number of tokens, the latencies, etc. Using all these graphs, I'll link this video from LangChain in the description. If you want to get into more details about the features, you can go through this video. I hope this video gave a lot of clarity on which tools are meant for what and when to use which. If you found this video helpful, please like and subscribe for more content like this. If you want me to create a video on a particular topic, please let me know in the comments. Thank you for watching and see you next time.